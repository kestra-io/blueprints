id: api-python-sql
namespace: company.team

tasks:
  - id: api
    type: io.kestra.plugin.core.http.Request
    uri: https://dummyjson.com/products

  - id: python
    type: io.kestra.plugin.scripts.python.Script
    taskRunner:
      type: io.kestra.plugin.scripts.runner.docker.Docker
    containerImage: python:slim
    beforeCommands:
      - pip install polars
    outputFiles:
      - products.csv
    script: |
      import polars as pl
      data = {{ outputs.api.body | jq('.products') | first }}
      df = pl.from_dicts(data)
      df.glimpse()
      df.select(["brand", "price"]).write_csv("products.csv")

  - id: sql_query
    type: io.kestra.plugin.jdbc.duckdb.Query
    inputFiles:
      in.csv: "{{ outputs.python.outputFiles['products.csv'] }}"
    sql: |
      SELECT brand, round(avg(price), 2) as avg_price
      FROM read_csv_auto('{{ workingDir }}/in.csv', header=True)
      GROUP BY brand
      ORDER BY avg_price DESC;
    fetchType: STORE

extend:
  title: Build an ETL Pipeline from a REST API Using Python, Polars, and DuckDB
  description: |
    This blueprint demonstrates how to build a **lightweight ETL pipeline**
    that extracts data from a **REST API**, processes it with **Python and
    Polars**, and analyzes it using **SQL with DuckDB**.

    It performs the following actions:

    - Fetches structured JSON data from a REST API.
    - Processes and transforms the data using Python inside a Docker container.
    - Uses Polars for fast, in-memory data manipulation and CSV generation.
    - Runs analytical SQL queries with DuckDB directly on the transformed data.
    - Stores and previews query results as a table for inspection and analysis.

    This pattern is ideal for **data engineering**, **API-driven analytics**,
    **ad-hoc ETL jobs**, and **lightweight data pipelines** where data needs to
    be extracted, transformed, and queried without setting up heavy
    infrastructure.

    Configuration:
      - Replace the API endpoint with your own REST API as needed.
      - Add authentication headers if working with private APIs.
      - Customize the Python transformation logic to fit your schema.
      - Modify the DuckDB SQL query to perform aggregations, filtering, or joins
        on the transformed data.

    By combining API ingestion, Python-based transformation, and SQL analytics
    in a single workflow, this blueprint provides a flexible foundation for
    modern ETL pipelines and exploratory data processing.
  tags:
    - Data
  ee: false
  demo: true
  meta_description: |
    Build an ETL pipeline from a REST API using Python, Polars, and DuckDB.
    Extract data, transform it in Python, and analyze it with SQL in a single
    automated workflow.

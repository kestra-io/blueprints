id: s3-dataset-assets
namespace: company.assets
description: Pull JSON from S3, unify into a dataset, process with DuckDB, and ship asset lineage with governed metadata.

inputs:
  - id: bucket
    type: STRING
    default: "my-json-data-bucket"
    description: S3 bucket containing the JSON files to ingest.
  - id: prefix
    type: STRING
    default: ""
    description: Optional prefix (folder) inside the bucket to scope downloads.
  - id: regexp
    type: STRING
    default: "^lead-.*\\.json$"
    description: Regex filter for the JSON files to include.
  - id: region
    type: STRING
    default: "eu-central-1"
    description: AWS region for the source bucket.

tasks:
  - id: download_json_from_s3
    type: io.kestra.plugin.aws.s3.Downloads
    description: Download JSON files from S3 and register the raw files as dataset assets.
    bucket: "{{ inputs.bucket }}"
    prefix: "{{ inputs.prefix }}"                  # set to a folder if your files live under one
    regexp: "{{ inputs.regexp }}"                  # matches your lead-001/2/3.json files
    region: "{{ inputs.region }}"
    action: NONE
    assets:
      outputs:
        - id: raw_json_files
          type: io.kestra.plugin.ee.assets.Dataset
          namespace: "{{ flow.namespace }}"
          metadata:
            location: "s3://{{ inputs.bucket }}/{{ inputs.prefix }}"
            format: json
            layer: raw

  - id: concat_dynamic_one
    type: io.kestra.plugin.core.storage.Concat
    description: Merge the downloaded JSON files into a single staging dataset.
    files: "{{ outputs.download_json_from_s3.outputFiles | jq('.[]') }}"
    assets:
      inputs:
        - id: raw_json_files
          type: io.kestra.plugin.ee.assets.Dataset
          namespace: "{{ flow.namespace }}"
      outputs:
        - id: unified_json_dataset
          type: io.kestra.plugin.ee.assets.Dataset
          namespace: "{{ flow.namespace }}"
          metadata:
            location: "{{ workingDir }}/unified-data.json"
            format: json
            layer: staging

  - id: duckdb_process_and_query
    type: io.kestra.plugin.jdbc.duckdb.Queries
    description: Load the unified JSON into DuckDB, create a curated table, and persist as a dataset asset.
    inputFiles:
      unified-data.json: "{{ outputs.concat_dynamic_one.uri }}"
    sql: |
      CREATE TABLE IF NOT EXISTS unified_dataset AS
      SELECT * FROM read_json('unified-data.json');
      INSERT INTO unified_dataset SELECT * FROM read_json('unified-data.json');
      SELECT * FROM unified_dataset;
    fetchType: STORE
    outputDbFile: true
    assets:
      inputs:
        - id: unified_json_dataset
          type: io.kestra.plugin.ee.assets.Dataset
          namespace: "{{ flow.namespace }}"
      outputs:
        - id: unified_json_dataset
          type: io.kestra.plugin.ee.assets.Dataset
          namespace: "{{ flow.namespace }}"
          metadata:
            location: "{{ workingDir }}/unified-leads.json"
            table: unified_dataset
            format: json
            layer: curated

  - id: ship_assets
    type: io.kestra.plugin.ee.assets.AssetShipper
    description: Export the curated dataset asset for downstream consumers.
    assetExporters:
      - id: file_exporter
        type: io.kestra.plugin.ee.assets.FileAssetExporter
        format: JSON

pluginDefaults:
  - type: io.kestra.plugin.aws
    values:
      accessKeyId: "{{ secret('AWS_ACCESS_KEY_ID') }}"
      secretKeyId: "{{ secret('AWS_SECRET_ACCESS_KEY') }}"
      region: "{{ inputs.region }}"
      allowFailure: false

extend:
  title: Register S3 JSON files as governed datasets and curate them with DuckDB
  description: |
    Download JSON files from S3, merge them into a unified dataset, curate with DuckDB, and register each stage as governed assets for lineage and reuse.

    Prerequisites:
      - Secrets:
        - AWS_ACCESS_KEY_ID: IAM access key with s3:GetObject permissions on the source bucket
        - AWS_SECRET_ACCESS_KEY: IAM secret key paired with the access key
      - Services:
        - An S3 bucket containing JSON files (defaults expect files named like `lead-001.json`)
    Quick start:
      1. Set the `bucket`, optional `prefix`, `regexp` filter, and `region` inputs to target your JSON files.
      2. Ensure AWS secrets are configured in Kestra (`AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`).
      3. Run the flow; it will download JSON files, merge them, process via DuckDB, and export curated assets.
      4. View the asset catalog to see raw, staging, and curated datasets registered with metadata (locations, layers, table).
    Expected outputs:
      - assets.raw_json_files: Dataset asset pointing to the raw S3 JSON files.
      - assets.unified_json_dataset (staging): Dataset asset for the merged JSON file.
      - assets.unified_json_dataset (curated): Dataset asset for the DuckDB table and curated file.
  tags:
    - Data
  ee: true
  demo: false
  meta_description: Merge JSON files from S3, curate them with DuckDB, and register raw, staging, and curated datasets as governed Kestra assets with lineage.

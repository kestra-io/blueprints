id: python-pandas-etl
namespace: community.blueprints
description: |
  # ETL with Python and Pandas
  This blueprint demonstrates how to simulate a classic ETL process:
  1. **Extract**: Simulate fetching rows from a database.
  2. **Transform**: Use `pandas` to aggregate data by category.
  3. **Load**: Output the aggregated metrics back to Kestra.

inputs:
  - id: row_count
    type: INT
    defaults: 100
    description: "Number of rows to generate"

tasks:
  - id: transform-data
    type: io.kestra.plugin.scripts.python.Script
    warningOnStdErr: false
    docker:
      image: ghcr.io/kestra-io/pydata:latest
    script: |
      import pandas as pd
      import numpy as np
      from kestra import Kestra

      # 1. EXTRACT (Simulated)
      # In a real scenario, you might use 'sqlalchemy' or 'cx_Oracle' here.
      print(f"Extracting {{ inputs.row_count }} rows...")

      df = pd.DataFrame({
          'id': range({{ inputs.row_count }}),
          'category': np.random.choice(['A', 'B', 'C'], {{ inputs.row_count }}),
          'value': np.random.uniform(10, 100, {{ inputs.row_count }})
      })

      # 2. TRANSFORM
      # Group by category and sum values
      summary = df.groupby('category')['value'].sum().reset_index()
      print("\nData Transformation Summary:")
      print(summary)

      # 3. LOAD (Output to Kestra)
      # Send the highest value back to Kestra outputs
      max_val = float(df['value'].max())
      Kestra.outputs({'max_value': max_val})

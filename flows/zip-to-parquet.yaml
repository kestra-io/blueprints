id: zip-to-parquet
namespace: company.team

variables:
  file_id: "{{ execution.startDate | dateAdd(-3, 'MONTHS') | date('yyyyMM') }}"

tasks:
  - id: get_zipfile
    type: io.kestra.plugin.core.http.Download
    uri: https://divvy-tripdata.s3.amazonaws.com/{{ render(vars.file_id) }}-divvy-tripdata.zip

  - id: unzip
    type: io.kestra.plugin.compress.ArchiveDecompress
    algorithm: ZIP
    from: "{{ outputs.get_zipfile.uri }}"

  - id: parquet_output
    type: io.kestra.plugin.scripts.python.Script
    taskRunner:
      type: io.kestra.plugin.scripts.runner.docker.Docker
    env:
      FILE_ID: "{{ render(vars.file_id) }}"
    inputFiles: "{{ outputs.unzip.files }}"
    dependencies:
      - pandas
      - pyarrow
    script: |
      import os
      import pandas as pd

      file_id = os.environ["FILE_ID"]
      file = f"{file_id}-divvy-tripdata.csv"

      df = pd.read_csv(file)
      df.to_parquet(f"{file_id}.parquet", index=False)
    outputFiles:
      - "*.parquet"

extend:
  shortDescription: "This ETL workflow automates the ingestion and transformation of compressed CSV datasets into the Parquet format using Python."
  title: Convert a zipped CSV file to Parquet using a Python ETL workflow
  description: |
    This ETL workflow automates the ingestion and transformation of compressed
    CSV datasets into the Parquet format using Python.

    The flow dynamically downloads a monthly Divvy Trip Data ZIP archive from a
    public HTTP endpoint, extracts the CSV file, and converts it to Parquet
    using Pandas inside a containerized Python task.

    This pattern is ideal for building repeatable data pipelines that prepare
    raw, compressed data for analytics, data lakes, or downstream warehouse
    ingestion where columnar formats such as Parquet are preferred.
  tags:
    - Core
  ee: false
  demo: true
  meta_description: Convert zipped CSV data into Parquet with a Python ETL pipeline. Download, extract, and transform compressed datasets into analytics-ready Parquet files.


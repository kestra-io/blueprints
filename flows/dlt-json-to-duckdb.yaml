id: dlt-json-to-duckdb
namespace: company.team

tasks:
  - id: dlt_pipeline
    type: io.kestra.plugin.scripts.python.Script
    taskRunner:
      type: io.kestra.plugin.scripts.runner.docker.Docker
    containerImage: python:3.11-slim
    outputFiles:
      - dummy_products.duckdb
    beforeCommands:
      - pip install dlt[duckdb]
    script: |
      import dlt
      import requests

      response = requests.get('https://dummyjson.com/products')
      response.raise_for_status()
      data = response.json()['products']

      pipeline = dlt.pipeline(
          pipeline_name='dummyjson_products_pipeline',
          destination='duckdb',
          dataset_name='products'
      )

      pipeline.run(data, table_name='product')

extend:
  shortDescription: "This flow shows how to extract data from a JSON-based REST API and load it into a DuckDB database using dlt (data load tool)."
  title: Build a Python pipeline to load JSON API data into DuckDB using dlt
  description: |
    This flow shows how to extract data from a JSON-based REST API and load it
    into a DuckDB database using dlt (data load tool).

    The entire ingestion logic is implemented in a single Python script that:
    - Fetches data from an external HTTP API
    - Extracts and normalizes the JSON payload
    - Loads the data directly into DuckDB using dltâ€™s built-in destination

    dlt automatically handles schema inference and table creation, making this
    approach well suited for lightweight ELT pipelines, local analytics, and
    rapid prototyping without manual database setup.

    The flow runs inside a Docker container and produces a DuckDB database file
    that can be queried immediately after execution.
  tags:
    - Data
  ee: false
  demo: true
  meta_description: Extract JSON data from an API and load it into DuckDB using dlt in a single Python-based data pipeline.


id: single-repo-ingestion
namespace: company.team

description: |
  **Single Repository Incremental Documentation Ingestion**
  
  Intelligent workflow that detects and processes only changed files from a GitHub repository into a MongoDB vector store for RAG operations. Can be triggered by the `repo-ingestion-controller` subflow or executed independently.
  
  **Key Features:**
  - **Change Detection**: Compares SHA hashes between GitHub and stored metadata
  - **Incremental Processing**: Only processes modified files, not entire repository
  - **Vector Embeddings**: Uses Nebius AI (BAAI/bge-en-icl) for semantic search capability
  - **Smart Cleanup**: Removes outdated document versions before ingesting new content
  - **Dual Execution**: Works as subflow dependency or standalone workflow
  
  **Process Flow:**
  1. Fetches previously ingested file metadata from MongoDB
  2. Retrieves current GitHub repository tree via API
  3. Identifies changed files by comparing SHA hashes
  4. Downloads and processes only modified files using LlamaIndex
  5. Generates embeddings and stores in MongoDB Atlas Vector Search
  6. Updates repository tracking metadata with ingestion status
  

inputs:
  - id: repo_name
    type: STRING
    defaults: "mindsdb/mindsdb"
  - id: branch
    type: STRING
    defaults: "main"

tasks:
  - id: fetch_file_list
    type: io.kestra.plugin.mongodb.Find
    connection:
      uri: "{{secret('MONGO_URI')}}"
    database: "{{secret('DB')}}"
    collection: "{{secret('COLLECTION_NAME')}}"
    filter:
      repo_name: "{{inputs.repo_name}}"
    projection:
      _id: 0

  - id: get_repo_tree
    type: io.kestra.plugin.scripts.python.Script
    beforeCommands:
      - pip install requests
    script: |
      from kestra import Kestra
      import requests

      api_url = f"https://api.github.com/repos/{{inputs.repo_name}}/git/trees/{{inputs.branch}}?recursive=1"
      headers = {}
      token = "{{secret('GITHUB_API_TOKEN')}}"
      if token:
          headers["Authorization"] = f"token {token}"
      resp = requests.get(api_url, headers=headers)
      resp.raise_for_status()
      tree = resp.json().get("tree", [])
      Kestra.outputs({'repo_files': [{'path': item['path'], 'sha': item['sha']} for item in tree if item['type'] == 'blob']})

  - id: filter_updated_files
    type: io.kestra.plugin.scripts.python.Script
    script: |
      from kestra import Kestra


      ingested_files = {{outputs.fetch_file_list.rows[0].files}}
      github_tree = {{outputs.get_repo_tree.vars.repo_files}}


      ingested_map = {f["path"]: f["sha"] for f in ingested_files}
      github_map = {f["path"]: f["sha"] for f in github_tree}

      diffs = []
      doc_ids = []

      for path in ingested_map:
          if path in github_map and ingested_map[path] != github_map[path]:
              diffs.append({"path": path, "sha": github_map[path]})
              doc_ids.append(f"{{inputs.repo_name}}:{{inputs.branch}}:{path}")

      # Output the results
      Kestra.outputs({
          'changed_files': diffs,
          'total_changes': len(diffs),
          'doc_ids': doc_ids,
          'has_changes': len(diffs) > 0,
          'file_paths': [f["path"] for f in diffs]
      })

  - id: if_has_changes
    type: io.kestra.plugin.core.flow.If
    condition: "{{ outputs.filter_updated_files.vars.has_changes }}"
    then:
      - id: fetch_file_content
        type: io.kestra.plugin.scripts.python.Script
        beforeCommands:
          - pip install github-file-loader
        outputFiles:
          - temp.json
        script: |
          import json
          from kestra import Kestra
          from github_file_loader import FileLoader

          file_loader = FileLoader("{{secret('GITHUB_API_TOKEN')}}")
          files_to_fetch = {{ outputs.filter_updated_files.vars.changed_files }}
          print(files_to_fetch)
          file_paths = [file["path"] for file in files_to_fetch]
          fetched_files, _ = file_loader.load_files_sync("{{inputs.repo_name}}", file_paths, "{{inputs.branch}}" )

          fetched_files_dict = [file.dict() for file in fetched_files]

          f = open("temp.json", "w")
          f.write(json.dumps(fetched_files_dict))
          f.close()

      - id: ingest_docs
        type: io.kestra.plugin.scripts.python.Script
        beforeCommands:
          - pip install llama-index==0.12.42 llama-index-embeddings-nebius==0.3.1 llama-index-llms-nebius==0.1.2 llama-index-vector-stores-mongodb==0.6.1
        script: |
          import os
          os.environ["TRANSFORMERS_VERBOSITY"] = "error"

          import json
          from datetime import datetime
          from kestra import Kestra
          from pymongo import MongoClient
          from llama_index.core import Document, Settings, StorageContext, VectorStoreIndex
          from llama_index.vector_stores.mongodb import MongoDBAtlasVectorSearch
          from llama_index.core.text_splitter import SentenceSplitter
          from llama_index.embeddings.nebius import NebiusEmbedding
          from llama_index.llms.nebius import NebiusLLM

          mongo_client = MongoClient('{{secret('MONGO_URI')}}')
          doc_collection = mongo_client['{{secret('DB')}}']['{{secret('VECTOR_COLLECTION_NAME')}}']
          repo_collection = mongo_client['{{secret('DB')}}']['{{secret('COLLECTION_NAME')}}']

          # Remove old ingested docs
          def remove_outdated_files(doc_ids):
            delete_count = 0
            for doc_id in doc_ids:
              result = doc_collection.delete_many({"metadata.doc_id":doc_id})
              delete_count += result.deleted_count

            return delete_count

          # Update repository information incrementally.
          def update_repo_info(repo_name, files_with_sha):
            existing_repo = repo_collection.find_one({"_id":repo_name})

            existing_files = existing_repo.get("files",[])
            existing_file_lookup = {f["path"]: f for f in existing_files}

            for file_info in files_with_sha:
              existing_file_lookup[file_info["path"]] = {
                "path": file_info["path"],
                "sha": file_info["sha"],
                "last_ingested": datetime.now(),
                "status": "ingested",
              }

            merged_files = list(existing_file_lookup.values())

            # Update only specific fields, don't replace entire document
            update_operation = {
                "$set": {
                    "files": merged_files,
                    "file_count": len(merged_files),  # Use actual count
                    "last_updated": datetime.now(),
                    "status": "complete",
                    "branch": "{{inputs.branch}}",
                    "tracking_enabled": True,
                }
            }
            repo_collection.update_one({"_id": repo_name}, update_operation)
            return True

          Settings.node_parser = SentenceSplitter(chunk_size=3072)
          Settings.llm = NebiusLLM(api_key="{{secret('NEBIUS_API_KEY')}}",model="meta-llama/Llama-3.3-70B-Instruct-fast")
          Settings.embed_model = NebiusEmbedding(api_key="{{secret('NEBIUS_API_KEY')}}", model_name="BAAI/bge-en-icl", embed_batch_size=25)

          f = open("{{outputs.fetch_file_content.outputFiles['temp.json']}}","r")
          content = f.read()
          files_to_ingest = json.loads(content)

          documents = []

          for file_data in files_to_ingest:
              # Extract file information
              file_path = file_data.get("path", "")
              file_name = file_data.get("name", "")
              file_extension = file_path.split(".")[-1] if "." in file_path else ""
              directory = "/".join(file_path.split("/")[:-1]) if "/" in file_path else ""
              
              # Create metadata dictionary
              metadata = {
                  "file_path": file_path,
                  "file_name": file_name,
                  "file_extension": file_extension,
                  "directory": directory,
                  "repo": "{{inputs.repo_name}}",
                  "branch": "{{inputs.branch}}",
                  "sha": file_data.get("sha", ""),
                  "size": file_data.get("size", 0),
                  "url": file_data.get("url", ""),
                  "raw_url": file_data.get("download_url", ""),
                  "type": file_data.get("type", "file"),
                  "encoding": file_data.get("encoding", ""),
              }
              
              # Create LlamaIndex Document
              document = Document(
                  text=file_data.get("content", ""),
                  doc_id=f"{{inputs.repo_name}}:{{inputs.branch}}:{file_path}",
                  metadata=metadata,
              )
              
              documents.append(document)

          deleted_count = remove_outdated_files({{outputs.filter_updated_files.vars.doc_ids}})

          vector_store = MongoDBAtlasVectorSearch(
              mongodb_client=mongo_client,
              db_name='docmcp',
              collection_name='doc_rag',
              vector_index_name='vector_index',
              fulltext_index_name='fts_index',
              embedding_key="embedding",
              text_key="text",
          )


          storage_context = StorageContext.from_defaults(vector_store=vector_store)

          VectorStoreIndex.from_documents(
              documents,
              storage_context=storage_context
          )

          update_repo_info('{{inputs.repo_name}}', {{outputs.filter_updated_files.vars.changed_files}})

          Kestra.outputs({
            'deleted_count': deleted_count
          })

      - id: log-ingestion-report
        type: io.kestra.plugin.core.log.Log
        message: "{{inputs.repo_name}} updated with {{outputs.filter_updated_files.vars.total_changes}} and removed old {{outputs.ingest_docs.vars.deleted_count}}"
    else:
      - id: log-repo-not-changed
        type: io.kestra.plugin.core.log.Log
        message: "{{inputs.repo_name}} has no changes"

